"""
class GraphConvInfo
	- old meta;)
	- will combine all samples in batch in single arrays (no more batch distinction for convs! will allow equal-size splitting if necessary)  ... so this class will be generated by some external logic
		- check if some node idx shared;)?
		- need deterministic ordering of nodes (to features)
	- edge features (possibly clustered/condensated -- logic here) 
		- these will be given to external function. it can split that by type,... I don't care here, it's an opaque array
		- but know what edge connects what
	- connectivity (idxs)
		-> for each output node: list of tuples (edge feat idx, src node idx)
		- opt arg: list of surviving nodes
"""
from __future__ import division
import six
import igraph
import torch
import math


    
# Input graph assumptions:
#   - nodes in 0:n
#   - directed graph, self-loops optional (will be added if necessary)

# edge_feat_func - receives list of edge attributes (dict), returns Tensor for edge features and LongTensor of inverse indices if edge clustering was performed (there are less edge features than edges, as some are used repeatedly)
    


    
    
class GraphConvInfo(object):          

    def __init__(self, *args, **kwargs):
        self._idxn = None           #index into source node features
        self._idxe = None           #index into edge features (or None if linear, i.e. no clustering)
        self._degrees = None        #in-degrees of output nodes
        self._edgefeats = None      #edge features
        if len(args)>0 or len(kwargs)>0:
            self.set_batch(*args, **kwargs)
      
    def set_batch(self, graphs, edge_feat_func):
            
        graphs = graphs if isinstance(graphs,(list,tuple)) else [graphs]
        p = 0
        idxn = []
        self._degrees = []
        edges = []
        uu = []
                
        for i,G in enumerate(graphs):
        
            indeg = G.indegree(G.vs, loops=True) # we silently assume that self-loops are in the graphs already
            
            for v in range(G.vcount()):               
                for u in G.predecessors(v):    # TODO: better iterate over edges
                    idxn.append(u+p)
                    edges.append(G.es[G.get_eid(u,v)].attributes())
                    
                self._degrees.append(indeg[v])        
                
            p += G.vcount()

        self._edgefeats, self._idxe = edge_feat_func(edges)
        
        self._idxn = torch.LongTensor(idxn)
        if self._idxe is not None:
            assert self._idxe.numel() == self._idxn.numel()
            
        
    def get_buffers(self, cuda):
        return (self._idxn.cuda() if cuda else self._idxn,
               self._idxe.cuda() if cuda and self._idxe is not None else self._idxe,
               self._degrees,               
               self._edgefeats.cuda() if cuda else self._edgefeats)
        # TODO: should cache internal cuda buffers so that convolutions using the same GI share data? But maybe wasting mem?
