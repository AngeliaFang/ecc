"""
class GraphConvInfo
	- old meta;)
	- will combine all samples in batch in single arrays (no more batch distinction for convs! will allow equal-size splitting if necessary)  ... so this class will be generated by some external logic
		- check if some node idx shared;)?
		- need deterministic ordering of nodes (to features)
	- edge features (possibly clustered/condensated -- logic here) 
		- these will be given to external function. it can split that by type,... I don't care here, it's an opaque array
		- but know what edge connects what
	- connectivity (idxs)
		-> for each output node: list of tuples (edge feat idx, src node idx)
		- opt arg: list of surviving nodes
"""
from __future__ import division
import six
import igraph
import torch
import math


    
# Input graph assumptions:
#   - nodes in 0:n
#   - directed graph, self-loops optional (will be added if necessary)

# edge_feat_func - receives list of edge attributes (dict), returns Tensor for edge features and LongTensor of inverse indices if edge clustering was performed (there are less edge features than edges, as some are used repeatedly)
    


    
    
class GraphConvInfo(object):          

    def __init__(self, *args, **kwargs):
        self._idxn = None           #index into source node features
        self._idxd = None           #index into dest node features (in the original graph; used for rebuilding adjacency matrix in coo format)
        self._idxe = None           #index into edge features (or None if linear, i.e. no clustering)
        self._degrees = None        #in-degrees of output nodes
        self._edgefeats = None      #edge features
        self._edges_degnorm = None  #multiplicative constant for each regressed weights (used to simulate normalized Laplacian weighting, 1/sqrt(deg(v)*deg(u))
        if len(args)>0 or len(kwargs)>0:
            self.set_batch(*args, **kwargs)

            
    def set_batch(self, graphs, edge_feat_func, edge_cache=None, self_loops=True, out_graphs=None):
               
        def degree_dicts(G):
            indeg = G.indegree(G.vs, loops=False)
            if self_loops:
                indeg = [d+1 for d in indeg]                    
            outdeg = G.outdegree(G.vs, loops=False)
            if self_loops:
                outdeg = [d+1 for d in outdeg]            
            return indeg, outdeg
            
        graphs = graphs if isinstance(graphs,(list,tuple)) else [graphs]
        p = 0
        idxn = [] #source nodes
        idxd = [] #dest nodes
        self._degrees = []
        edges = []
        edges_id = []
        edges_degnorm = []
                
        for i,G in enumerate(graphs):
        
            nodes = G.vs
            map = dict(zip(nodes, range(p, p+G.vcount())))
            indeg, outdeg = degree_dicts(G)
            
            for node in nodes:
                #if out_graphs is not None and node not in out_graphs[i]: #strided convolution (compute only vertices present in out_graphs)
                #    continue

                if self_loops:
                    idxn.append(map[node]) #self-loop (create if necessary)
                    idxd.append(map[node])
                    edges_id.append((i,node,node))         
                    edges.append(G.es[G.get_eid(node,node)].attributes() if G.get_eid(node,node,error=False)>=0 else {'self_loop':1})
                    #edges_degnorm.append(1.0/math.sqrt(indeg[node]*outdeg[node]))
                
                for u in G.predecessors(node):
                    v=node
                    if u==v: continue
                    idxn.append(map[G.vs[u]])
                    idxd.append(map[v])
                    edges_id.append((i,u,v))
                    edges.append(G.es[G.get_eid(u,v)].attributes())
                    #edges_degnorm.append(1.0/math.sqrt(indeg[v]*outdeg[u])) #~normalized Laplacian of unweighted graph
                    
                self._degrees.append(indeg[node.index])        
                
            p += G.vcount()

        if edge_cache is not None:
            self._edgefeats, self._idxe = edge_cache.select(edges_id)
        else:
            self._edgefeats, self._idxe = edge_feat_func(edges)
          
        self._edges_degnorm = torch.Tensor(edges_degnorm)
        self._idxn = torch.LongTensor(idxn)
        self._idxd = torch.LongTensor(idxd)
        if self._idxe is not None:
            assert self._idxe.numel() == self._idxn.numel()
            
        
    def get_buffers(self, cuda):
        return (self._idxn.cuda() if cuda else self._idxn,
               self._idxe.cuda() if cuda and self._idxe is not None else self._idxe,
               self._degrees,               
               self._edgefeats.cuda() if cuda else self._edgefeats)
        # TODO: should cache internal cuda buffers so that convolutions using the same GI share data? But maybe wasting mem?
